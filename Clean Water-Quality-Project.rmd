---
title: "Water Quality"
output: html_document
---


## Introduction

Arsenic naturally occurs in groundwater sources around the world. Arsenic contamination of groundwater affects millions of people around the world including the United States, Nicaragua, Argentina, China, Mexico, Chile, Bangladesh, India, and Vietnam, for example (Smith et al. 2000; Amini et al. 2008; Lin et al. 2017). The World Health Organization (WHO 2018a) estimates that over 140 million people in 50 countries are exposed to arsenic contaminated drinking water above the WHO guideline of 10 $\mu$g/L. Health effects of arsenic exposure include numerous types of cancer and other disorders.

This project follows an analysis of a public health study performed in rural Bangladesh (Gelman et al. 2004). In this study, wells used for drinking water were analyzed for arsenic contamination and correspondingly labeled as safe or unsafe. The study determined whether households switched the well used for drinking water and measured. Additionally, several variables were measured that were thought to possibly influence the decision of whether or not to switch wells. Here, we will investigate how accurately we can predict whether or not a household will switch wells based on these environmental variables.

This analysis will look at three modeling techniques, logistic regression, XGBoost, and k nearest neighbors. We will analyze the accuracy of the models to pick the best model for predicting whether a household will switch wells based on the level of arsenic, distance to next closest safe well, whether they are associated with community organizations, and the highest level of education in the household.

## Data Collection

See Gelman et al. (2004) for a discussion of data collection. Briefly, arsenic levels were measured in Araihazar, Bangladesh during the years 1999 - 2000. Additional information was collected by a survey:
1. Whether or not the household switched wells.
2. The distance (in meters) to the closest known safe well.
3. Whether any members of the household are involved in community organizations.
4. The highest education level in the household.

### Load necessary packages

```{r, warning=FALSE}

#skimr provides a nice summary of a data set
library(skimr)
#tidyverse contains packages we will use for processing and plotting data
library(tidyverse)
#GGally has a nice pairs plotting function
library(GGally)
#tidymodels has a nice workflow for many models. We will use it for XGBoost
library(tidymodels)
#xgboost lets us fit XGBoost models
library(xgboost)
#vip is used to visualize the importance of predicts in XGBoost models
library(vip)

library(ggplot2)
library(reshape2)
library("kknn")

#Set the plotting theme
theme_set(theme_bw())

```


## Data Preparation


### Load the data 


**$\rightarrow$ Load the data set contained in the file `wells.dat` and name the data frame `df`.**

```{r}
df <- read.table("wells.dat")
```


We want to predict whether a household switches wells. We will use the data set to create a model of how these factors effect a household's decision to switch to a new well or remain at the same well.


#### Rename the columns

The names of the columns in this data frame are understandable, but two of the columns, `switch` and `distance`, have the names of functions that already exist in R. It is bad practice to name your variables or functions after existing functions, so we will change them. While we are at it, we will change some other names to be complete words.

```{r}
df <- df %>% 
  rename(switch_well = "switch",
         distance = "dist",
         association = "assoc",
         education = "educ")
```


#### Convert data types for qualitative predictor

**$\rightarrow$ Use the `mutate` function to convert `switch_well` and `association` to factors.**

Note that all variables are coded as numeric variables, but `switch_well` and `association` are categorical variables that happen to be coded using 0 and 1. We will convert these variables to factors.

```{r}
df <- df %>% 
  mutate(switch_well = factor(switch_well)) %>% 
  mutate(association = factor(association))
```

## Compare models

We will use logistic regression, XGBoost, and k-nearest neighbors to construct models that predict the probability of switching wells.

To compare the different approaches, we will use a training and testing split of the data set.

We will use the tidymodels approach for all models.

### Get train and test splits

We will split the data into training and testing sets, with 80% of the data kept for training.   

```{r}
set.seed(12)
split <- initial_split(df, prop = 0.8, strata = switch_well)

df_train <- training(split)
df_test <- testing(split)
```


### Logistic regression model

#### Model specification

**$\rightarrow$ First specify a logistic regression model with the glm engine.**

```{r}
log_reg_model <- logistic_reg() %>%
  set_engine("glm")
```


#### Workflow

**$\rightarrow$ Create a workflow that specifies the model formula to fit and add the model specification.**

```{r}
log_reg_wf <- workflow() %>%
  add_formula(switch_well ~ .) %>%
  add_model(log_reg_model)
```


#### Fit to training data

Fit the model to the training data and explore the coefficients.

**$\rightarrow$ First fit the model.**

```{r}
log_reg_fit <- log_reg_wf %>% 
  fit(df_train)
```

#### Predict test data

**$\rightarrow$ Generate predictions and bind the predictions together with the true `switch_well` values from the test data.**

```{r}
predictions_log_reg <- log_reg_fit %>%
  predict(new_data = df_test) %>% 
  bind_cols(df_test %>% dplyr::select(switch_well))
```

Binding the predictions and actual values together into one tibble will help us to plot the confusion matrix and to compute measures of accuracy.

#### Assess fit

**$\rightarrow$ Get prediction accuracy, sensitivity, and specificity. The prediction accuracy is equal to the proportion of correct predictions in the test data set. Sensitivity is the proportion of correct predictions for households that did switch wells. Specificity is the proportion of correct predictions for households that did not switch wells.**

```{r}
log_acc <- predictions_log_reg %>%
  metrics(switch_well, .pred_class) %>%
  dplyr::select(-.estimator) %>%
  filter(.metric == "accuracy") %>% 
  mutate(.estimate = round(.estimate,3))

log_sens <- predictions_log_reg %>%
  sens(switch_well, .pred_class, event_level = "second") %>%
  dplyr::select(-.estimator) %>%
  mutate(.estimate = round(.estimate,3)) 

log_spec <- predictions_log_reg %>%
  spec(switch_well, .pred_class, event_level = "second") %>%
  dplyr::select(-.estimator) %>%
  mutate(.estimate = round(.estimate,3))
```

Accuracy: `r log_acc$.estimate`

Sensitivity: `r log_sens$.estimate`

Specificity: `r log_spec$.estimate` 

### XGBoost

#### Set up the model

The model will be a boosted tree model, so we start by specifying the features of a `boost_tree` model. The`boost_tree` creates a specification of a model, but does not fit the model.


**$\rightarrow$ First specify an XGBoost model for classification with the xgboost engine. Set`tree_depth`, `min_n`, `loss_reduction`, `sample_size`, `mtry`, and `learn_rate` as parameters to tune. Set `trees` = 1000.**


```{r}
xgb_model <- boost_tree(
  mode = "classification",  #We are solving a classification problem
  trees = 1000, 
  tree_depth = tune(),  # tune() says that we will specify this parameter later
  min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune(),                         
  ) %>% 
  set_engine("xgboost") ## We will use xgboost to fit the model
```

**$\rightarrow$ Create a workflow that specifies the model formula and the model type. We are still setting up the model; this does not fit the model.**


```{r}
xgb_wf <- workflow() %>%
  add_formula(switch_well ~ .) %>%
  add_model(xgb_model)
```


#### Fit the model

**$\rightarrow$ Specify the parameter grid using the function `grid_latin_hypercube`:**

We need to fit all of the parameters that we specified as `tune()`. 

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 30  #Create 30 sets of the 6 parameters
)
```


**$\rightarrow$ Create folds for cross-validation, using stratified sampling based on `switch_well`.**

```{r}
folds <- vfold_cv(df_train, strata = switch_well)
```


**$\rightarrow$ Do the parameter fitting.** 

```{r}
xgb_grid_search <- tune_grid(
  xgb_wf,              #The workflow
  resamples = folds,   #The training data split into folds
  grid = xgb_grid,     #The grid of parameters to fit
  control = control_grid(save_pred = TRUE)
)
```


**$\rightarrow$ Get the best model based on `accuracy`.**

```{r}
best_xgb <- select_best(xgb_grid_search, "accuracy")
```


**$\rightarrow$ Update the workflow with the best parameters.**

```{r}
final_xgb <- finalize_workflow(
  xgb_wf,
  best_xgb
)
```


#### Fit to training data

**$\rightarrow$ Fit the model to the training data.**


```{r}
xgb_fit <- final_xgb %>% 
  fit(df_train)
```


#### Predict test data

**$\rightarrow$ Generate predictions and bind them together with the true values from the test data.**

```{r}
predictions_xgb <- xgb_fit %>%
  predict(new_data = df_test) %>% 
  bind_cols(df_test %>% dplyr::select(switch_well))
```


#### Assess fit

**$\rightarrow$ Get prediction accuracy, sensitivity, and specificity. The prediction accuracy is equal to the proportion of correct predictions in the test data set. Sensitivity is the proportion of correct predictions for households that did switch wells. Specificity is the proportion of correct predictions for households that did not switch wells.**

```{r}
xgb_acc <- predictions_xgb %>%
  metrics(switch_well, .pred_class) %>%
  dplyr::select(-.estimator) %>%
  filter(.metric == "accuracy") %>% 
  mutate(.estimate = round(.estimate,3))

xgb_sens <- predictions_xgb %>%
  sens(switch_well, .pred_class, event_level = "second") %>%
  dplyr::select(-.estimator) %>%
  mutate(.estimate = round(.estimate,3)) 

xgb_spec <- predictions_xgb %>%
  spec(switch_well, .pred_class, event_level = "second") %>%
  dplyr::select(-.estimator) %>%
  mutate(.estimate = round(.estimate,3))
```
 
Accuracy: `r xgb_acc$.estimate`  

Sensitivity: `r xgb_sens$.estimate`

Specificity: `r xgb_spec$.estimate`

### k nearest neighbors

#### Model specification

First specify a k nearest neighbors model with the kknn engine.

```{r}
knn_model <- nearest_neighbor(
    mode = "classification",
    neighbors = tune("K")
  ) %>%
  set_engine("kknn")
```


#### Workflow

Create a workflow that specifies the model formula to fit and the model type.

```{r}

knn_wf <- workflow() %>%
  add_formula(switch_well~.) %>%
  add_model(knn_model) 

```


#### Fit the hyperparameter k

Specify a set of values of k to try.
```{r}

knn_grid <- parameters(knn_wf) %>%  
  update(K = neighbors(c(1, 50))) %>% 
  grid_latin_hypercube(size = 10)

```


Use cross validation on the previously defined folds to find the best value of k.

```{r}

knn_grid_search <- tune_grid(
  knn_wf,
  resamples = folds,
  grid = knn_grid,
  control = control_grid(save_pred = TRUE)
)

```



Get the best model based on `accuracy`.

```{r}

best_knn <- select_best(knn_grid_search, "accuracy")

```


Update the workflow with the best parameter k.

```{r}
final_knn <- finalize_workflow(
          knn_wf,
          best_knn
)
```


#### Fit to training data

Fit the model to the training data and explore the coefficients.

First fit the model.
```{r}
knn_fit <- final_knn %>% 
  fit(df_train) 
```


#### Predict test data

Generate predictions and bind together with the true values from the test data.
```{r}

predictions_knn <- knn_fit %>%
  predict(new_data = df_test) %>%  
  bind_cols(df_test %>% dplyr::select(switch_well))

```


#### Assess fit

**$\rightarrow$ Get prediction accuracy, sensitivity, and specificity. The prediction accuracy is equal to the proportion of correct predictions in the test data set. Sensitivity is the proportion of correct predictions for households that did switch wells. Specificity is the proportion of correct predictions for households that did not switch wells.**

```{r}

knn_acc <- predictions_knn %>%
  metrics(switch_well, .pred_class) %>%
  dplyr::select(-.estimator) %>%
  filter(.metric == "accuracy") %>% 
  mutate(.estimate = round(.estimate,3))

knn_sens <- predictions_knn %>%
  sens(switch_well, .pred_class, event_level = "second") %>%
  dplyr::select(-.estimator) %>%
  mutate(.estimate = round(.estimate,3)) 


knn_spec <- predictions_knn %>%
  spec(switch_well, .pred_class, event_level = "second") %>%
  dplyr::select(-.estimator) %>%
  mutate(.estimate = round(.estimate,3))
```

Accuracy: `r knn_acc$.estimate` 

Sensitivity: `r knn_sens$.estimate`

Specificity: `r knn_spec$.estimate` 

### Compare models

You used three methods to construct a model

1. Logistic regression
2. XGBoost
3. k nearest neighbors

Compare the performance of the models. 

*Accuracy* is the proportion of correct predictions for the data set.

*Sensitivity* is the proportion of correct predictions for households that switched wells.

*Specificity* is the proportion of correct predictions for households that didn't switch wells.

|Model|Accuracy|Sensitivity|Specificity|
|:--|:--|:--|:--|
|Logistic Regression|`r log_acc$.estimate` |`r log_sens$.estimate` |`r log_spec$.estimate` |
|XGBoost|`r xgb_acc$.estimate` |`r xgb_sens$.estimate` |`r xgb_spec$.estimate` |
|k nearest neighbors|`r knn_acc$.estimate` |`r knn_sens$.estimate` |`r knn_spec$.estimate` |

XGBoost has both the highest accuracy and sensitivity, but the lowest specificity. In all three models the specificity is low, meaning that the models aren't good at predicting whether a household will remain at their current well. On the other hand, the sensitivities are pretty high, thus our models are better at predicting when a switch will occur. Since we are more concerned with accuracy and sensitivity, XGBoost is the best model for predicting a switch.

## Conclusion

In summary, we tested 3 modeling methods: logistic regression, XGBoost, and k nearest neighbors. To compare these models, we looked at accuracy, sensitivity, and specificity. From these figures, we could tell that the three models are sensitive, meaning that they predict when a household will switch wells much better than whether they won't switch. This is good for answering the original question because it is valuable to know what influences a household to switch wells.

In the XGBoost model arsenic is the most important predictor in the model, and distance is a close second. This means that high arsenic levels are the main driver for a household switching wells, but the distance to the nearest safe well also has a strong influence. Education and community association have less of an effect on whether a household will switch wells.

I believe that we have sufficiently answered the original question of determining how well we can predict if a household will switch wells. The model can predict if a household will switch with `r xgb_sens$.estimate` accuracy. 

